{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versão Vetorizada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(\\hat{w})=\\frac{1}{N}(y-\\hat{\\mathbf{w}}^T\\mathbf{x})^T(y-\\hat{\\mathbf{w}}^T\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,X,Y):\n",
    "    #Y - Y_chapeu (predicao)\n",
    "    res = Y - np.dot(X,w)\n",
    "    totalError = np.dot(res.T,res)    \n",
    "    return totalError / float(len(Y))#media da soma dos erros ao quadrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current,X,Y,learningRate):\n",
    "    #Y - Y_chapeu (predicao)\n",
    "    res = Y - np.dot(X,w_current)\n",
    "    \n",
    "    #novo gradiente\n",
    "    new_grad = np.multiply(-2/len(X),np.dot(X.T, res)) #usando a mesma ideia do vídeo do lab passado, multiplicando por -2/N\n",
    "     \n",
    "    #novos valores dos parametros\n",
    "    new_w = np.zeros((len(new_grad),1))\n",
    "    for i in range(len(new_grad)):\n",
    "        new_w[i,0] = w_current[i] - (learningRate * new_grad[i])\n",
    "        \n",
    "    return [new_w,new_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf,np.inf, np.inf,np.inf,np.inf,np.inf])\n",
    "    i = 0\n",
    "    while (np.linalg.norm(grad)>=epsilon):\n",
    "        w, grad = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at w0 = [0.], w1 = [0.], w2 = [0.], w3 = [0.], w4 = [0.], w5 = [0.], error = [[54.47995386]]\n",
      "Running...\n",
      "MSE na iteração 0 é de [[48.42204544]]\n",
      "MSE na iteração 1000 é de [[0.47897163]]\n",
      "MSE na iteração 2000 é de [[0.46681063]]\n",
      "Gradiente descendente convergiu com w0 = [0.03378612], w1 = [0.16605965], w2 = [0.20216232], w3 = [0.19079395], w4 = [0.28206898], w5 = [0.10360042], error = [[0.46059511]]\n",
      "Versão vetorizada rodou em: 108.09540748596191 ms\n"
     ]
    }
   ],
   "source": [
    "points = np.genfromtxt(\"sample_treino.csv\", delimiter=\",\", skip_header=1)\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "X = points[:,:6]\n",
    "Y = points[:,6][:,np.newaxis]\n",
    "\n",
    "init_w = np.zeros((6,1))\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "epsilon = 0.285\n",
    "print(\"Starting gradient descent at w0 = {0}, w1 = {1}, w2 = {2}, w3 = {3}, w4 = {4}, w5 = {5}, error = {6}\".format(init_w[0], init_w[1],init_w[2], init_w[3],init_w[4], init_w[5], compute_mse_vectorized(init_w,X,Y)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, w2 = {2}, w3 = {3}, w4 = {4}, w5 = {5}, error = {6}\".format(w[0], w[1],w[2], w[3],w[4], w[5], compute_mse_vectorized(w,X,Y)))\n",
    "print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando os coeficientes usando o Scikit Learn (código disponível no notebook RegressaoMultiplaScikit.ipynb), obtém-se os seguintes resultados:\n",
    "w0 = 0.58891358 , w1 = 0.1723595  , 0.13712469 , w3 = 0.15369677 , w4 = 0.33858381 , w5 = 0.05214724, error = 0.46\n",
    "\n",
    "Para poder aproximar o erro desta solução à do Scikit, usei o epsilon = 0.285 (após alguns testes para definir o mais próximo).\n",
    "Pode-se ver que os valores não são iguais, mas se aproximam bastante, tendo apenas o w0 como mais destoante, porém o obtido foi menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
